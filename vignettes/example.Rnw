%%\VignetteIndexEntry{crmPack: Object-oriented implementation of CRM designs}
%%\VignetteKeywords{dose escalation, CRM, N-CRM, EWOC, Bayesian adaptive design}
%%\VignettePackage{crmPack}
%%\VignetteDepends{crmPack,ggmcmc,grid}

\documentclass[UKenglish]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[UKenglish]{babel}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{url}
\usepackage[sumlimits, intlimits, namelimits]{amsmath} % Math
\usepackage{amssymb}                                   % Symbols
\usepackage[caption = true]{subfig}
\usepackage[longnamesfirst,round]{natbib}
\usepackage{tikz}
\usepackage[nogin]{mySweave}            % use the customized Sweave style


\begin{document}

%% reduce the height of the plots
\setkeys{Gin}{width=0.65\textwidth}

%% no indentation of paragraphs
\noindent

\title{Using the package \texttt{crmPack}:\\ introductory examples}
\author{Daniel Saban\'es Bov\'e}
\date{\today\\Package version 0.0.23}
\maketitle

This short vignette shall introduce into the usage of the package
\texttt{crmPack}. Hopefully it makes it easy for you to set up your own CRM.

If you have any questions or feedback concerning the package, please write an
email to me:
\href{mailto:sabanesd@roche.com}{\nolinkurl{sabanesd@roche.com}}
or even better post it on the Hive page:
\url{https://roche.jiveon.com/projects/crmpack}
Thank you very much in advance!

<<setup, echo=FALSE, results=hide>>=
options(continue="  ")                  # use two blanks instead of "+" for
                                        # continued lines (for easy copying)
@

\section{Installation}
\label{sec:installation}

Probably you have already installed the package. If not, please refer to the
separate installation document available from the Hive page:
\url{https://roche.jiveon.com/servlet/JiveServlet/download/38-91276/installation.docx}

\texttt{crmPack} is relying on \href{http://mcmc-jags.sourceforge.net/}{JAGS}
(please click on the link for going to the webpage of the project) for the
internal MCMC computations. WinBUGS is no longer required. To increase the speed
of the MCMC sampling, \texttt{BayesLogit} is currently used for the
\texttt{LogisticNormal} model.

\section{Getting started}
\label{sec:getting-started}

Before being able to run anything, you have to load the package with
<<load>>=
library(crmPack)
@

For browsing the help pages for the package, it is easiest to start the web
browser interface with
<<webinterface, eval=FALSE>>=
crmPackHelp()
@
This gives you the list of all help pages available for the package.

First, we will set up a logistic normal model. Therefore, you can now click on
the corresponding help page \texttt{LogisticLogNormal-class} as background
information for the next steps.

\section{Model setup}
\label{sec:model-setup}

\subsection{Logistic model with bivariate (log) normal prior}
\label{sec:logist-norm-model}

Let us start with setting up the logistic regression model, which will then be
used in the following for the continual reassessment method. With the following
command, we create a new model of class \texttt{LogisticLogNormal}, with certain
mean and covariance prior parameters and reference dose:

<<model-setup>>=
model <- LogisticLogNormal(mean=c(-0.85, 1),
                           cov=
                               matrix(c(1, -0.5, -0.5, 1),
                                      nrow=2),
                           refDose=56)
@

The \texttt{R}-package \texttt{crmPack} uses the S4 class system for
implementation of the dose-escalation designs. There is the convention that
class initialization functions have the same name as the class, and all class
names are capitalized. We can query the class that an object belongs to with the
\texttt{class} function:

<<class>>=
class(model)
@

We can look in detail at the structure of \texttt{model} as follows:

<<str>>=
str(model)
@

We see that the object has \Sexpr{length(slotNames(model))} slots, and their
names. These can be accessed with the \verb|@| operator (similarly as for lists
the \verb|$| operator), for example we can extract the \texttt{dose} slot:

<<dose>>=
model@dose
@

This is the function that computes for given parameters \texttt{alpha0} and
\texttt{alpha1} the dose that gives the probability \texttt{prob} for a
dose-limiting toxicity (DLT). You can find out yourself about the other slots,
by looking at the help page for \texttt{Model-class} in the help browser,
because all models are just special cases of the general \texttt{Model} class.
In the \texttt{Model-class} help page, you also find out that there are four
additional specific model classes that are subclasses of the \texttt{Model}
class, namely \texttt{LogisticLogNormalSub}, \texttt{LogisticNormal},
\texttt{LogisticKadane} and \texttt{DualEndpoint}.

\subsection{Advanced model specification}
\label{sec:advanc-model-spec}

There are a few further, advanced ways to specify a model object in
\texttt{crmPack}.

First, a minimal informative prior \citep{Neuenschwander2008} can be computed
using the \texttt{MinimalInformative} function. The construction is based on the
input of a minimal and a maximal dose, where certain ranges of DLT probabilities
are deemed unlikely. A logistic function is then fitted through the
corresponding points on the dose-toxicity plane in order to derive Beta
distributions also for doses in-between. Finally these Beta distributions are
approximated by a common \texttt{LogisticNormal} (or \texttt{LogisticLogNormal})
model. So the minimal informative construction avoids explicit specification of
the prior parameters of the logistic regression model.

In our example, we could construct it as follows, assuming a minimal dose of 0.1
mg and a maximum dose of 100 mg:

<<min-inf>>=
set.seed(432)
coarseGrid <- c(0.1, 10, 30, 60, 100)
minInfModel <- MinimalInformative(dosegrid = coarseGrid,
                                  refDose=50,
                                  threshmin=0.2,
                                  threshmax=0.3,
                                  control=
                                  list(threshold.stop=0.03,
                                       maxit=200))
@

We use a few grid points between the minimum and the maximum to guide the
approximation routine, which is based on a stochastic optimization method (the
\texttt{control} argument is for this optimization routine, please see the help
page for \texttt{Quantiles2LogisticNormal} for details). Therefore we need to
set a random number generator seed beforehand to be able to reproduce the
results in the future. The \texttt{threshmin} and \texttt{threshmax} values
specify the probability thresholds above and below, respectively, it is very
unlikely (only 5\% probability) of the probability of DLT at the minimum and
maximum dose, respectively.

The result \texttt{minInfModel} is a list, and we can use its contents to
illustrate the creation of the prior:

<<min-inf-res, fig=TRUE>>=
matplot(x=coarseGrid,
        y=minInfModel$required,
        type="b", pch=19, col="blue", lty=1,
        xlab="dose",
        ylab="prior probability of DLT")
matlines(x=coarseGrid,
         y=minInfModel$quantiles,
         type="b", pch=19, col="red", lty=1)
legend("right",
       legend=c("quantiles", "approximation"),
       col=c("blue", "red"),
       lty=1,
       bty="n")
@

In this plot we see in blue the quantiles (2.5\%, 50\%, and 97.5\%) of
the Beta distributions that we approximate with the red quantiles of the
logistic normal model. We see that the distance is quite small, and the maximum
distance between any red and blue point is:

<<min-inf-dist>>=
minInfModel$distance
@

The final approximating model, which has produced the red points, is contained
in the \texttt{model} list element:

<<min-inf-model>>=
str(minInfModel$model)
@

Here we see in the slots \texttt{mean}, \texttt{cov} the parameters that have
been determined. At this point a slight warning: you cannot directly change
these parameters in the slots of the existing model object, because the
parameters have also been saved invisibly in other places in the model object.
Therefore, always use the class initialization function to create a new model
object, if new parameters are required. But if we want to further use the
approximation model, we can save it under a shorter name, e.g.:

<<min-inf-model-extract>>=
myModel <- minInfModel$model
@

\section{Data}
\label{sec:data}

If you are in the middle of a trial and you would like to recommend the next
dose, then you have data from the previous patients for input into the model.
This data needs to be captured in a \texttt{Data} object. For example:

<<data>>=
data <- Data(x=c(0.1, 0.5, 1.5, 3, 6, 10, 10, 10),
             y=c(0, 0, 0, 0, 0, 0, 1, 0),
             cohort=c(0, 1, 2, 3, 4, 5, 5, 5),
             doseGrid=
                 c(0.1, 0.5, 1.5, 3, 6,
                   seq(from=10, to=80, by=2)))
@

Most important are \texttt{x} (the doses) and \texttt{y} (the DLTs, 0 for no DLT
and 1 for DLT), as well as the dose grid \texttt{doseGrid}. All computations are
using the dose grid specified in the \texttt{Data} object. So for example,
except for patient number 7, all patients were free of DLTs.

Again, you can find out the details in the help page \texttt{Data-class}. Note
that you have received a warning here, because you did not specify the patient
IDs -- however, automatic ones just indexing the patients have been created for
you:

<<ids>>=
data@ID
@

You can get a visual summary of the data by applying \texttt{plot} to the
object:\footnote{Note that for all \texttt{plot} calls in this vignette, you can
  leave away the wrapping \texttt{print} function call if you are working
  interactively with \texttt{R}. It is only because of the \texttt{Sweave}
  production of this vignette that the \texttt{print} statement is needed.}

<<plotdata, fig=TRUE>>=
print(plot(data))
@

\section{Obtaining the posterior}
\label{sec:obtaining-posterior}

As said before, \texttt{crmPack} relies on MCMC sampling for obtaining the
posterior distribution of the model parameters, given the data. The MCMC
sampling can be controlled with an object of class \texttt{McmcOptions}, created
for example as follows:

<<mcmc-opts>>=
options <- McmcOptions(burnin=100,
                       step=2,
                       samples=2000)
@

Now the object \texttt{options} specifies that you would like to have 2000
parameter samples obtained from a Markov chain that starts with a ``burn-in''
phase of 100 iterations that are discarded, and then save a sample every 2
iterations. Note that these numbers are too low for actual implementation and
only used for illustrating purposes here; normally you would specify at least
the default parameters of the initialization function \texttt{McmcOptions}:
$10\,000$ burn-in iterations and $10\,000$ samples saved every 2nd iteration.
You can look these up in help browser under the link ``McmcOptions''.

After having set up the options, you can proceed to MCMC sampling by calling the
\texttt{mcmc} function:

<<mcmc-sampling>>=
set.seed(94)
samples <- mcmc(data, model, options)
@

The \texttt{mcmc} function takes the data object, the model and the MCMC
options. By default, JAGS is used for obtaining the samples. Use the option
\texttt{verbose=TRUE} to show a progress bar and detailed JAGS messages.

You could specify \texttt{program="WinBUGS"} to use WinBUGS instead. This will
dynamically try to load the \texttt{R}-package \texttt{R2WinBUGS} to interface
with WinBUGS. Therefore you must have installed both WinBUGS and the R-package
\texttt{R2WinBUGS} in order to use this option.

Finally, it is good practice to check graphically that the Markov chain has
really converged to the posterior distribution. To this end, \texttt{crmPack}
provides an interface to the convenient \texttt{R}-package \texttt{ggmcmc}. With
the function \texttt{get} you can extract the individual parameters from the
object of class \texttt{Samples}. For example, we extract the $\alpha_{0}$
samples:

<<mcmc-extract>>=
## look at the structure of the samples object:
str(samples)
## now extract the alpha0 samples (intercept of the regression model)
alpha0samples <- get(samples, "alpha0")
@

\texttt{alpha0samples} now contains the $\alpha_{0}$ samples in a format
understood by \texttt{ggmcmc} and we can produce plots with it, e.g. a trace plot
and an autocorrelation plot:

<<ggmcmc, fig=TRUE>>=
library(ggmcmc)
print(ggs_traceplot(alpha0samples))
@

<<ggmcmc2, fig=TRUE>>=
print(ggs_autocorrelation(alpha0samples))
@

So here we see that we have some autocorrelation in the samples, and might
consider using a higher thinning parameter in order to decrease it.

You can find other useful plotting functions in the package information:

<<ggmcmc-help, eval=FALSE>>=
help(package="ggmcmc", help_type="html")
@

\section{Plotting the model fit}
\label{sec:plot-fit}

After having obtained the parameter samples, we can plot the model fit, by
supplying the samples, model and data to the generic plot function:

<<plot-model-fit, fig=TRUE>>=
print(plot(samples, model, data))
@

This plot shows the posterior mean curve and 95\% equi-tailed credible intervals
at each point of the dose grid from the \texttt{data} object.

Note that you can also produce a plot of the prior mean curve and credible
intervals, i.e. from the model without any data. This works in principle the
same way as with data, just that we use an empty data object:

<<empty-data, fig=TRUE>>=
## provide only the dose grid:
emptydata <- Data(doseGrid=data@doseGrid)
## obtain prior samples with this Data object
priorsamples <- mcmc(emptydata, model, options)
## then produce the plot
print(plot(priorsamples, model, emptydata))
@

\section{Escalation Rules}
\label{sec:escalation-rules}

For the dose escalation, there are four kinds of rules:
\begin{enumerate}
\item \texttt{Increments}: For specifying maximum allowable increments between
  doses
\item \texttt{NextBest}: How to derive the next best dose
\item \texttt{CohortSize}: For specifying the cohort size
\item \texttt{Stopping}: Stopping rules for finishing the dose escalation
\end{enumerate}
We have listed here the classes of these rules, and there are multiple
subclasses for each of them, which you can find as links in the help pages
\texttt{Increments-class}, \texttt{NextBest-class}, \texttt{CohortSize-class}
and \texttt{Stopping-class}.

\subsection{Increments rules}
\label{sec:increments-rules}

Let us start with looking in detail at the increments rules. Currently two
specific rules are implemented: Maximum relative increments based on the current
dose (\texttt{IncrementsRelative}), and maximum relative increments based on the
current cumulative number of DLTs that have happened
(\texttt{IncrementsRelativeDLT}).

For example, in order to specify maximum increase of 100\% for doses up to 20 mg,
and 33\% for doses above 20 mg, we can setup the following increments rule:

<<rel-incs>>=
myIncrements <- IncrementsRelative(intervals=c(0, 20),
                                   increments=c(1, 0.33))
@

Here the \texttt{intervals} slot specifies the left bounds of the intervals, in
which the maximum relative \texttt{increments} (note: decimal values here, no
percentages!) are valid.

The increments rule is used by the \texttt{maxDose} function to obtain the
maximum allowable dose given the current data:

<<max-dose>>=
nextMaxDose <- maxDose(myIncrements,
                       data=data)
nextMaxDose
@

So in this case, the next dose could not be larger than 20 mg.

\subsection{Rules for next best dose recommendation}
\label{sec:rules-next-best}

There are two implemented rules for toxicity endpoint CRMs: \texttt{NextBestMTD}
that uses the posterior distribution of the MTD estimate (given a target
toxicity probability defining the MTD), and \texttt{NextBestNCRM} that
implements the N-CRM, using posterior probabilities of target-dosing and
overdosing at the dose grid points to recommend a next best dose.

For example, in order to use the N-CRM with target toxicity interval from 20\%
to 35\%, and a maximum overdosing probability of 25\%, we specify:

<<ncrm-spec>>=
myNextBest <- NextBestNCRM(target=c(0.2, 0.35),
                           overdose=c(0.35, 1),
                           maxOverdoseProb=0.25)
@

Alternatively, we could use an MTD driven recommendation rule. For example, with
a target toxicity rate of 33\%, and recommending the 25\% posterior quantile of
the MTD, we specify

<<mtd-spec>>=
mtdNextBest <- NextBestMTD(target=0.33,
                           derive=
                               function(mtdSamples){
                                   quantile(mtdSamples, probs=0.25)
                               })
@

Note that the \texttt{NextBestMTD} class is quite flexible, because you can
specify a function \texttt{derive} that derives the next best dose from the
posterior MTD samples.

During the study, in order to derive the next best dose, we supply the generic
\texttt{nextBest} function with the rule, the maximum dose, the posterior
samples, the model and the data:

<<next-best-run>>=
doseRecommendation <- nextBest(myNextBest,
                               doselimit=nextMaxDose,
                               samples=samples, model=model, data=data)
@

The result is a list with two elements: \texttt{value} contains the numeric
value of the recommended next best dose, and \texttt{plot} contains a plot that
illustrates how the next best dose was computed. In this case we used the N-CRM
rule, therefore the plot gives the target-dosing and overdosing probabilities
together with the safety bar of 25\%, the maximum dose and the final
recommendation (the red triangle):

<<next-best-results, fig=TRUE>>=
doseRecommendation$value
print(doseRecommendation$plot)
@

\subsection{Cohort size rules}
\label{sec:increments-rules}

Similarly to the increments rules, you can define intervals in the dose space
and/or the DLT space to define the size of the cohorts. For example, let's
assume we want to have one patient only in the cohorts until we reach 30 mg or
the first DLT is encountered, and then proceed with three patients per cohort.

We start by creating the two separate rules, first for the dose range:

<<size-range>>=
mySize1 <- CohortSizeRange(intervals=c(0, 30),
                           cohortSize=c(1, 3))
@

Then for the DLT range:

<<size-dlt>>=
mySize2 <- CohortSizeDLT(DLTintervals=c(0, 1),
                         cohortSize=c(1, 3))
@

Finally we combine the two rules by taking the maximum number of patients of
both rules:

<<size-combined>>=
mySize <- maxSize(mySize1, mySize2)
@

The \texttt{CohortSize} rule is used by the \texttt{size} function, together
with the next dose and the current data, in order to determine the size of the
next cohort:

<<size-eval>>=
size(mySize,
     dose=doseRecommendation$value,
     data=data)
@

Because we have one DLT already, we would go for 3 patients for the next cohort.

Moreover, if you would like to have a constant cohort size, you can use the
following \texttt{CohortSizeConst} class, which we will use (with three
patients) for simplicity for the remainder of this vignette:

<<size-const>>=
mySize <- CohortSizeConst(size=3)
@


\subsection{Stopping rules}
\label{sec:stopping-rules}

Stopping rules are often quite complex, and built from an and/or combination of
multiple parts. Therefore the \texttt{crmPack} implementation mirrors this, and
multiple atomic stopping rules can be combined easily. For example, let's assume
we would like to stop the trial if there are at least 3 cohorts and at least
50\% probability in the target toxicity interval $(20\%, 35\%)$, or the maximum
sample size of 20 patients has been reached. Then we start by creating the three
pieces the rule is composed of:

<<rules-bits>>=
myStopping1 <- StoppingMinCohorts(nCohorts=3)
myStopping2 <- StoppingTargetProb(target=c(0.2, 0.35),
                                  prob=0.5)
myStopping3 <- StoppingMinPatients(nPatients=20)
@

Finally we combine these with the ``and'' operator \texttt{\&} and the ``or''
operator \verb-|-:

<<rules-compose>>=
myStopping <- (myStopping1 & myStopping2) | myStopping3
@

You can find a link to all implemented stopping rule parts in the help page
\texttt{Stopping-class}.

During the study, any (atomic or combined) stopping rule can be used by the
function \texttt{stopTrial} to determine if the rule has been fulfilled. For
example in our case:

<<rules-try>>=
stopTrial(stopping=myStopping, dose=doseRecommendation$value,
          samples=samples, model=model, data=data)
@

We receive here \texttt{FALSE}, which means that the stopping rule criteria have
not been met. The attribute \texttt{message} contains the textual results of the
atomic parts of the stopping rule. Here we can read that the probability for
target toxicity was just 30\% for the recommended dose 20 mg and therefore too
low, and also the maximum sample size has not been reached, therefore the trial
shall continue.

\section{Simulations}
\label{sec:simulations}

In order to run simulations, we first have to build a specific design, that
comprises a model, the escalation rules, starting data, a cohort size (currently
fixed during the trial) and a starting dose. It might seem strange at first
sight that we have to supply starting data to the design, but we will show below
that this makes sense. First, we use our \texttt{emptydata} object that only
contains the dose grid, and a cohorts of 3 patients, starting from 0.1 mg:

<<design-setup>>=
design <- Design(model=model,
                 nextBest=myNextBest,
                 stopping=myStopping,
                 increments=myIncrements,
                 cohortSize=mySize,
                 data=emptydata,
                 startingDose=3)
@

\subsection{Simulating from a true scenario}
\label{sec:simulating-from-true}

Next, we have to define a true scenario, from which the data should arise. In
this case, this only requires a function that computes the probability of DLT
given a dose. Here we use a specific case of the function contained in the model
space:

<<true-def, fig=TRUE>>=
## define the true function
myTruth <- function(dose)
{
    model@prob(dose, alpha0=7, alpha1=8)
}

## plot it in the range of the dose grid
curve(myTruth(x), from=0, to=80, ylim=c(0, 1))
@

Now we can proceed to the simulations. We only generate 100 trial outcomes here
for illustration, for the actual study this should be increased of course to at
least 500:

<<run-sims>>=
time <- system.time(mySims <- simulate(design,
                                       args=NULL,
                                       truth=myTruth,
                                       nsim=100,
                                       seed=819,
                                       mcmcOptions=options,
                                       parallel=TRUE))[3]
time
@

We have wrapped the call to \texttt{simulate} in a \texttt{system.time} to
obtain the required time for the simulations (about \Sexpr{round(time)}~seconds
in this case). The argument \texttt{args} could contain additional arguments for
the \texttt{truth} function, which we did not require here and therefore let it
at the default \texttt{NULL}. We specify the number of simulations with
\texttt{nsim} and the random number generator seed with \texttt{seed}. Note that
we also pass again the MCMC options object, because during the trial simulations
the MCMC routines are used. Finally, the argument \texttt{parallel} can be used
to enable the use of all processors of the computer for running the simulations
in parallel. This can yield a meaningful speedup, especially for larger number
of simulations.

As (almost) always, the result of this call is again an object with a class, in
this case \texttt{Simulations}:

<<sim-class>>=
class(mySims)
@

From the help page

<<sim-help>>=
help("Simulations-class", help="html")
@

we see that this class is a subclass of the ``GeneralSimulations'' class. By
looking at the help pages for ``Simulations'' and the parent class
``GeneralSimulations'', we can find the description of all slots of
\texttt{mySims}. In particular, the \texttt{data} slot contains the list of
produced \texttt{Data} objects of the simulated trials. Therefore, we can plot
the course of e.g. the third simulated trial as follows:

<<third-trial, fig=TRUE>>=
print(plot(mySims@data[[3]]))
@

The final dose for this trial was

<<third-dose>>=
mySims@doses[3]
@

and the stopping reason was

<<third-stop>>=
mySims@stopReasons[[3]]
@

Furthermore, with this object, we can apply two methods. First, we can plot it,
i.e. we can apply the plot method:

<<sim-plot, fig=TRUE>>=
print(plot(mySims))
@

The resulting plot shows on the top panel a summary of the trial trajectories.
On the bottom, the proportions of doses tried, averaged over the simulated
trials, are shown. Note that you can select the plots by changing the
\texttt{type} argument of \texttt{plot}, which by default is \texttt{type =
  c("trajectory", "dosesTried")}.

Second, we can summarize the simulation results. Here again we have to supply a
true dose-toxicity function. We take the same (\texttt{myTruth}) as above:

<<sim-summary>>=
summary(mySims,
        truth=myTruth)
@

Note that sometimes the observed toxicity rate at the dose most often selected
(here 20 mg) is not available, because it can happen that no patients were
actually treated that dose during the simulations. (Here it is available.) This
illustrates that the MTD can be selected based on the evidence from the data at
other dose levels -- which is an advantage of model-based dose-escalation
designs.

Now we can also produce a plot of the summary results, which gives a bit more
detail than the textual summary we have just seen:

<<sim-sum-plot, fig=TRUE>>=
simSum <- summary(mySims,
                  truth=myTruth)
print(plot(simSum))
@

The top left panel shows the distribution of the sample size across the
simulated trials. In this case the trials had between 15 and 21 patients. The
top right panel shows the distribution of the final MTD estimate / recommended
dose across the simulated trials. The middle left panel shows the distribution
across the simulations of the DLT proportions observed in the patients dosed.
Here in most trials between 20 and 30\% of the patients had DLTs.
The middle right panel shows the distribution across simulations of the number
of patients treated above the target toxicity window (here we used the default
from 20\% to 35\%). Finally, in the bottom panel we see a comparison
of the true dose-toxicity curve (black) with the estimated dose-toxicity curves,
averaged (continuous red line) across the trials and with 95\% credible interval
across the trials. Here we see that the steep true dose-toxicity curve is not
recovered by the model fit.

If we find that e.g. the top right plot with the distribution of the final
selected doses is too small and shows not the right x-axis window, we can only
plot this one and add x-axis customization on top: (see the \texttt{ggplot2}
documentation for more information on customizing the plots)

<<sim-sum-plot2, fig=TRUE>>=
dosePlot <- plot(simSum, type="doseSelected") +
      scale_x_continuous(breaks=10:30, limits=c(10, 30))
print(dosePlot)
@

\subsection{Predicting the future course of the trial}
\label{sec:pred-future-course}

By simulating parameters from their current posterior distribution instead of an
assumed truth, it is possible to generate trial simulations from the posterior
predictive distribution at any time point during the trial. This means that we
can predict the future course of the trial, given the current data. In our
illustrating example, this would work as follows.

The rationale of the \texttt{simulate} call is now that we specify as the
\texttt{truth} argument the \texttt{prob} function from our assumed model, which
has additional arguments (in our case \texttt{alpha0} and \texttt{alpha1}) on
top of the first argument \texttt{dose}:

<<explain-fut>>=
model@prob
@

For the simulations, these arguments are internally given by the values
contained in the data frame given to \texttt{simulate} as the \texttt{args}
argument. In our case, we want to supply the posterior samples of
\texttt{alpha0} and \texttt{alpha1} in this data frame. We take only 50 out of
the 2000 posterior samples in order to reduce the runtime for this example:

<<fut-samples>>=
postSamples <- as.data.frame(samples@data)[(1:20)*50, ]
postSamples
@

Therefore, each simulated trial will come from a posterior sample of our
estimated model, given all data so far.

Furthermore we have to make a new \texttt{Design} object that contains the
current data to start from, and the current recommended dose as the starting dose:

<<design-future>>=
nowDesign <- Design(model=model,
                    nextBest=myNextBest,
                    stopping=myStopping,
                    increments=myIncrements,
                    cohortSize=mySize,
                    ## use the current data:
                    data=data,
                    ## and the recommended dose as the starting dose:
                    startingDose=doseRecommendation$value)
@

Finally we can execute the simulations:

<<sim-future>>=
time <- system.time(futureSims <- simulate(
    ## supply the new design here
    nowDesign,
    ## the truth is the assumed prob function
    truth=model@prob,
    ## further arguments are the
    ## posterior samples
    args=postSamples,
    ## do exactly so many simulations as
    ## we have samples
    nsim=nrow(postSamples),
    seed=918,
    ## this remains the same:
    mcmcOptions=options,
    parallel=TRUE))[3]
time
@

And now, exactly in the same way as above for the operating characteristics
simulations, we can summarize the resulting predictive simulations, for example
show the predicted trajectories of doses:

<<sim-future-plot, fig=TRUE>>=
print(plot(futureSims))
@

In the summary, we do not need to look at the characteristics involving the true
dose-toxicity function, because in this case we are not intending to compare the
performance of our CRM relative to a truth:

<<sim-future-summary>>=
summary(futureSims,
        truth=myTruth)
@

We see here e.g. that the estimated number of patients overall is 19, so 11 more
than the current 8 patients are expected to be needed before finishing the
trial.

\section{Simulating 3+3 design outcomes}
\label{sec:simul-3+3-design}

While \texttt{crmPack} focuses on model-based dose-escalation designs, it now
includes the 3+3 design in order to allow for convenient comparisons. Note that
actually no simulations would be required for the 3+3 design, because all
possible outcomes can be enumerated, however we still rely here on simulations
for consistency with the overall \texttt{crmPack} design.

The easiest way to setup a 3+3 design is the function
\texttt{ThreePlusThreeDesign}:

<<three-plus-three-setup>>=
threeDesign <- ThreePlusThreeDesign(doseGrid=c(5, 10, 15, 25, 35, 50, 80))
class(threeDesign)
@

We have used here a much coarser dose grid than for the model-based design
before, because the 3+3 design cannot jump over doses. The starting dose is
automatically chosen as the first dose from the grid. The outcome is a
\texttt{RuleDesign} object, and you have more setup options if you directly use
the \texttt{RuleDesign()} initialization function. We can then simulate trials,
again assuming that the \texttt{myTruth} function gives the true dose-toxicity
relationship:

<<three-sims>>=
threeSims <- simulate(threeDesign,
                      nsim=1000,
                      seed=35,
                      truth=myTruth,
                      parallel=TRUE)
@

As before for the model-based design, we can summarize the simulations:

<<three-sims-summary>>=
threeSimsSum <- summary(threeSims,
                        truth=myTruth)
threeSimsSum
@

Here we see that \Sexpr{threeSimsSum@doseMostSelected} mg was the dose most
often selected as MTD, and this is actually too low when comparing with the
narrow target dose interval going from
\Sexpr{round(threeSimsSum@targetDoseInterval[1],1)} to
\Sexpr{round(threeSimsSum@targetDoseInterval[2],1)} mg. This is an inherent problem of
dose-escalation designs where the dose grid has to be coarse: you might not know
before starting the trial which is the range where you need a more refined dose
grid. In this case we obtain doses that are too low, as one can see from the
average true toxicity of
\Sexpr{round(mean(threeSimsSum@toxAtDosesSelected)*100)}~\% at doses selected.
Graphical summaries are again obtained by calling ``plot'' on the summary
object:

<<three-sims-plot, fig=TRUE>>=
print(plot(threeSimsSum))
@

\section{Dual-endpoint designs}
\label{sec:dual-endp-designs}

The latest version of \texttt{crmPack} includes dual-endpoint designs. These are
still under development, and so far we have not yet published a peer-reviewed paper
on these methods, therefore please consider them as experimental.

In the help page ``DualEndpoint-class'' the general model structure is
described, and on the \href{https://roche.jiveon.com/projects/crmpack}{Hive
  page} more information can be found. Basically the idea is that a (single)
biomarker variable is the second endpoint of the dose-escalation design, with
the aim to maximize the biomarker response while controlling toxicity in a safe
range. This is useful when it can not be assumed that just increasing the dose
will always lead to a better clinical response.

Let's look at the data structure. Here is an example:
<<dual-data-struct>>=
data <- DataDual(
    x=
        c(0.1, 0.5, 1.5, 3, 6, 10, 10, 10,
          20, 20, 20, 40, 40, 40, 50, 50, 50),
    y=
        c(0, 0, 0, 0, 0, 0, 1, 0,
          0, 1, 1, 0, 0, 1, 0, 1, 1),
    w=
        c(0.31, 0.42, 0.59, 0.45, 0.6, 0.7, 0.55, 0.6,
          0.52, 0.54, 0.56, 0.43, 0.41, 0.39, 0.34, 0.38, 0.21),
    doseGrid=
        c(0.1, 0.5, 1.5, 3, 6,
          seq(from=10, to=80, by=2)))
@

The corresponding plot can again be obtained with:

<<dual-data-plot, fig=TRUE>>=
print(plot(data))
@

Here we see that there seems to be a maximum biomarker response at around 10~mg
already. In order to model this data, we consider a dual-endpoint model with RW1
structure for the dose-biomarker relationship:

<<dual-rw1-model>>=
model <- DualEndpointRW(mu=c(0, 1),
                        Sigma=matrix(c(1, 0, 0, 1), nrow=2),
                        sigma2betaW=
                        0.01,
                        sigma2W=
                        c(a=0.1, b=0.1),
                        rho=
                        c(a=1, b=1),
                        smooth="RW1")
@

We use a smoothing parameter $\sigma^{2}_{\beta_{W}} = 0.01$, an inverse-gamma
prior $\mathrm{IG}(0.1, 0.1)$ on the biomarker variance $\sigma^{2}_{W}$ and a
uniform prior (or $\mathrm{Beta}(1, 1)$ prior) on the correlation $\rho$ between
the latent DLT and the biomarker variable.

As the dual-endpoint models are more complex, it is advisable to use a
sufficiently long Markov chain for fitting them. Here we set:

<<dual-options>>=
options <- McmcOptions(burnin=20000,
                       step=3,
                       samples=10000)
@

Then we can obtain the MCMC samples:

<<dual-mcmc>>=
samples <- mcmc(data, model, options)
@

And we check the convergence by picking a few of the fitted biomarker means and
plotting their traceplots:

<<dual-conv, fig=TRUE>>=
data@nGrid
betaWpicks <- get(samples, "betaW", c(2, 10, 25))
ggs_traceplot(betaWpicks)
@

Here all 3 $\beta_{W,j}$ ($j=2, 10, 25$) means, which are the biomarker means
at the second, 10th and 25th gridpoint, respectively, seem to have
converged, as the traceplots show. (Remember that \verb|data@nGrid| gives the
number of gridpoints.) So we can plot the model fit:

\setkeys{Gin}{width=\textwidth}
<<dual-modelfit, fig=TRUE>>=
print(plot(samples, model, data, extrapolate=FALSE))
@
\setkeys{Gin}{width=0.65\textwidth}

We specify \texttt{extrapolate = FALSE} to focus the biomarker plot in the right
panel on the observed dose range, so we don't want to extrapolate the
biomarker fit to higher dose levels. We can also look at the estimated biomarker
precision $1 / \sigma^{2}_{W}$. For that we extract the precision ``precW'' and
then use another \texttt{ggmcmc} function to create the histogram:

<<dual-variance, fig=TRUE>>=
ggs_histogram(get(samples, "precW"))
@

For the selection of the next best dose, a special class
``NextBestDualEndpoint'' has been implemented. It tries to maximize the
biomarker response, under an NCRM-type safety constraint. If we want to have at
least 90\% of the maximum biomarker response, and a 25\% maximum overdose
probability for the next dose, we specify:

<<dual-nextbest>>=
myNextBest <- NextBestDualEndpoint(target=0.9,
                                   overdose=c(0.35, 1),
                                   maxOverdoseProb=0.25)
@

In our example, and assuming a dose limit of 50~mg given by the maximum
allowable increments, the next dose can then be found as follows:

<<dual-nextdose-eval>>=
nextDose <- nextBest(myNextBest,
                     doselimit=50,
                     samples=samples,
                     model=model,
                     data=data)
nextDose$value
@

A corresponding plot can be produced by printing the ``plot'' element of the
returned list:

<<dual-nextdose-plot>>=
print(nextDose$plot)
@

Here the bottom panel shows (as for the NCRM) the overdose probability, and we
see that doses above 6~mg are too toxic. In the top panel, we see the
probability for each dose to reach at least 90\% of the maximum biomarker
response in the dose grid --- this is here our target probability. While the
numbers are low, we clearly see that there is a local maximum at 10~mg of the
target probability, confirming what we have seen in the previous data and
model fit plots.

A corresponding stopping rule exists. When we have a certain probability to be
above a relative biomarker target, then the ``StoppingTargetBiomarker'' rule
gives back \texttt{TRUE} when queried if it has been fulfilled by the
\texttt{stopTrial} function. For example, if we require at least 50\%
probability to be above 90\% biomarker response, we specify:

<<dual-stop>>=
myStopping4 <- StoppingTargetBiomarker(target=0.9,
                                       prob=0.5)
@

In this case, the rule has not been fulfilled yet, as we see here:

<<dual-stop-try>>=
stopTrial(myStopping4, dose=nextDose$value,
          samples, model, data)
@

Again, this dual-endpoint specific rule can be combined as required with any
other stopping rule. For example, we could combine it with a maximum sample size
of 40~patients:

<<dual-stop-whole>>=
myStopping <- myStopping4 | StoppingMinPatients(40)
@

If one or both of the stopping rules are fulfilled, then the trial is stopped.

Let's try to build a corresponding dual-endpoint design. We start with an empty
data set, and use the relative increments rule defined in a previous section and
use a constant cohort size of 3 patients:

<<dual-design>>=
emptydata <- DataDual(doseGrid=data@doseGrid)
design <- DualDesign(model=model,
                     data=emptydata,
                     nextBest=myNextBest,
                     stopping=myStopping,
                     increments=myIncrements,
                     cohortSize=CohortSizeConst(3),
                     startingDose=6)
@

In order to study operating characteristics, we need to determine true biomarker
and DLT probability functions. Here we are going to use a biomarker function
from the beta family. Note that there is a corresponding ``DualEndpointBeta''
model class, that allows to have dual-endpoint designs with the beta biomarker
response function. Have a look at the corresponding help page for more
information on that. But let's come back to our scenario definition:

<<dual-scenario>>=
betaMod <- function (dose, e0, eMax, delta1, delta2, scal)
{
    maxDens <- (delta1^delta1) * (delta2^delta2)/((delta1 + delta2)^(delta1 + delta2))
    dose <- dose/scal
    e0 + eMax/maxDens * (dose^delta1) * (1 - dose)^delta2
}
trueBiomarker <- function(dose)
{
    betaMod(dose, e0=0.2, eMax=0.6, delta1=5, delta2=5 * 0.5 / 0.5, scal=100)
}
trueTox <- function(dose)
{
    pnorm((dose-60)/10)
}
@

We can draw the corresponding curves:

<<dual-sc-plot, fig=TRUE>>=
par(mfrow=c(1, 2))
curve(trueTox(x), from=0, to=80)
curve(trueBiomarker(x), from=0, to=80)
@

So the biomarker response peaks at 50~mg, where the toxicity is still low. After
deciding for a true correlation of $\rho=0$ and a true biomarker variance of
$\sigma^{2}_{W} = 0.01$ (giving a high signal-to-noise ratio), we can start
simulating trials (starting each with 6~mg):

<<dual-sims>>=
mySims <- simulate(design,
                   trueTox=trueTox,
                   trueBiomarker=trueBiomarker,
                   sigma2W=0.01,
                   rho=0,
                   nsim=10,
                   parallel=TRUE,
                   seed=3,
                   startingDose=6,
                   mcmcOptions =
                       McmcOptions(burnin=1000,
                                   step=1,
                                   samples=3000))
@

Note that we are having a ``small'' MCMC option set here, in order to reduce
simulation time --- for the real application, this should be ``larger''.

Plotting the result gives not only an overview of the final dose recommendations
and trial trajectories, but also a summary of the biomarker variance and
correlation estimates in the simulations:

<<dual-sims-plot, fig=TRUE>>=
print(plot(mySims))
@

Finally, a summary of the simulations can be obtained with the corresponding function:

<<dual-sims-sum>>=
sumOut <- summary(mySims,
                  trueTox=trueTox,
                  trueBiomarker=trueBiomarker)
sumOut
@

We see here that all trials proceeded until the maximum sample size of 40
patients (reaching 42 because of the cohort size 3). The doses selected are
lower than the toxicity target range, because here we were aiming for a
biomarker target instead, and the true biomarker response peaked at 50~mg.

The corresponding plot looks as follows:

<<dual-sim-sum-plot, fig=TRUE>>=
print(plot(sumOut))
@

We see that the average biomarker fit is not too bad in the range up to 50~mg,
but the toxicity curve fit is bad --- probably a result of the very low
frequency of DLTs. Again the warning here: the dual-endpoint designs are still
experimental!

\bibliographystyle{plainnat}
\bibliography{example}

\end{document}



%%% Local Variables:
%%% mode: latex-math
%%% TeX-master: t
%%% coding: utf-8-unix
%%% ispell-local-dictionary: "american"
%%% End:
